{"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":9774184,"sourceType":"datasetVersion","datasetId":5987267},{"sourceId":9774647,"sourceType":"datasetVersion","datasetId":5987603},{"sourceId":9779388,"sourceType":"datasetVersion","datasetId":5990982}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 0. Load necessary libraries","metadata":{"id":"tH1u-bhYAPb0"}},{"cell_type":"code","source":"#!/usr/bin/env python\n\n'''TODO: add high-level description of this Python script'''\n\nimport re\nimport json\n# import argparse <- don't need this in colab\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.initializers import Constant\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, precision_score, recall_score\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, TextVectorization, Bidirectional\nfrom tensorflow.keras.initializers import Constant\nimport tensorflow as tf\n\n# Set random seeds for reproducibility\nimport random as python_random","metadata":{"id":"xFkbQChnAYmj","execution":{"iopub.status.busy":"2024-11-02T14:01:35.808820Z","iopub.execute_input":"2024-11-02T14:01:35.809082Z","iopub.status.idle":"2024-11-02T14:01:49.346052Z","shell.execute_reply.started":"2024-11-02T14:01:35.809051Z","shell.execute_reply":"2024-11-02T14:01:49.345293Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1. Set up reproducble random seeds","metadata":{"id":"vQgQnZQlAfL2"}},{"cell_type":"code","source":"# Make reproducible as much as possible\nnp.random.seed(1234)\ntf.random.set_seed(1234)\npython_random.seed(1234)","metadata":{"id":"JpLaKrgdAqzD","execution":{"iopub.status.busy":"2024-11-02T14:01:49.347660Z","iopub.execute_input":"2024-11-02T14:01:49.348149Z","iopub.status.idle":"2024-11-02T14:01:49.353776Z","shell.execute_reply.started":"2024-11-02T14:01:49.348117Z","shell.execute_reply":"2024-11-02T14:01:49.352946Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 2. Path definitions\nArgparth is not supported in colab, which is replaced by direct path definitions","metadata":{"id":"5zckyCtRAzmU"}},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/off-set-rnn/train.tsv'\nDEV_PATH = '/kaggle/input/off-set-rnn/dev.tsv'\nTEST_PATH = '/kaggle/input/off-set-rnn/test.tsv'\nEMBEDDINGS_PATH = '/kaggle/input/glove-embedding/glove.twitter.27B.200d.txt'","metadata":{"id":"qXEi_kHnH8ax","execution":{"iopub.status.busy":"2024-11-02T14:01:49.354931Z","iopub.execute_input":"2024-11-02T14:01:49.355319Z","iopub.status.idle":"2024-11-02T14:01:49.380600Z","shell.execute_reply.started":"2024-11-02T14:01:49.355271Z","shell.execute_reply":"2024-11-02T14:01:49.379821Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 3. Corpus loading","metadata":{"id":"2DWaRTKkCaT-"}},{"cell_type":"code","source":"# Add preprocessing function to clean tweets\ndef clean_tweet(text):\n    # Convert to lowercase\n    #text = text.lower()\n\n    # Replace user mentions\n    text = re.sub(r'@[A-Za-z0-9_]+', '<user>', text)\n\n    # Replace URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '<url>', text)\n\n    # Replace hashtags but keep the text\n    text = re.sub(r'#([A-Za-z0-9_]+)', r'\\1', text)\n\n    # Replace repeated characters (e.g., 'hellooooo' -> 'helloo')\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n\n    return text\n","metadata":{"id":"IOz3O5emSLIV","execution":{"iopub.status.busy":"2024-11-02T14:01:49.382878Z","iopub.execute_input":"2024-11-02T14:01:49.383485Z","iopub.status.idle":"2024-11-02T14:01:49.390903Z","shell.execute_reply.started":"2024-11-02T14:01:49.383435Z","shell.execute_reply":"2024-11-02T14:01:49.390026Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def read_corpus(corpus_file):\n    '''Read in dataset and returns docs and labels'''\n    documents = []\n    labels = []\n\n    print(f\"Reading corpus from {corpus_file}\")\n\n    try:\n        # Read the TSV file\n        df = pd.read_csv(corpus_file, sep='\\t', header=None)\n\n        # Extract text and labels\n        documents = df[0].apply(clean_tweet).tolist()  # First column is text\n        labels = df[1].tolist()     # Second column is label\n\n        print(f\"Read {len(documents)} documents\")\n        print(f\"Label distribution: {pd.Series(labels).value_counts()}\")\n\n    except Exception as e:\n        print(f\"Error reading corpus: {str(e)}\")\n        return [], []\n\n    return documents, labels","metadata":{"id":"zOrP55xSCjRZ","execution":{"iopub.status.busy":"2024-11-02T14:01:49.391959Z","iopub.execute_input":"2024-11-02T14:01:49.392324Z","iopub.status.idle":"2024-11-02T14:01:49.401211Z","shell.execute_reply.started":"2024-11-02T14:01:49.392282Z","shell.execute_reply":"2024-11-02T14:01:49.400445Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 4. Load GloVe embedding","metadata":{"id":"3byLFY0wCmrH"}},{"cell_type":"code","source":"def read_embeddings(embeddings_file):\n    '''Read in word embeddings from file and save as numpy array'''\n    embeddings = json.load(open(embeddings_file, 'r'))\n    return {word: np.array(embeddings[word]) for word in embeddings}\n\ndef read_embeddings_from_txt(embeddings_file):\n    '''Read in word embeddings from a text file and save as numpy array'''\n    embeddings = {}\n    with open(embeddings_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.array(values[1:], dtype=float)\n            embeddings[word] = vector\n    return embeddings","metadata":{"id":"NXefrNGICmPq","execution":{"iopub.status.busy":"2024-11-02T14:01:49.402092Z","iopub.execute_input":"2024-11-02T14:01:49.402628Z","iopub.status.idle":"2024-11-02T14:01:49.414570Z","shell.execute_reply.started":"2024-11-02T14:01:49.402597Z","shell.execute_reply":"2024-11-02T14:01:49.413774Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_emb_matrix(voc, emb):\n    '''Get embedding matrix given vocab and the embeddings'''\n    num_tokens = len(voc) + 2\n    word_index = dict(zip(voc, range(len(voc))))\n    # Bit hacky, get embedding dimension from the word \"the\"\n    embedding_dim = len(emb[\"the\"])\n    # Prepare embedding matrix to the correct size\n    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = emb.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    # Final matrix with pretrained embeddings that we can feed to embedding layer\n    return embedding_matrix","metadata":{"id":"HEQADkUwC2tC","execution":{"iopub.status.busy":"2024-11-02T14:01:49.415539Z","iopub.execute_input":"2024-11-02T14:01:49.415838Z","iopub.status.idle":"2024-11-02T14:01:49.423550Z","shell.execute_reply.started":"2024-11-02T14:01:49.415807Z","shell.execute_reply":"2024-11-02T14:01:49.422806Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 5. Set up model","metadata":{"id":"-lp-mr3HCySu"}},{"cell_type":"code","source":"def create_model(Y_train, emb_matrix):\n    '''Create the Keras model to use'''\n    # Define settings, you might want to create cmd line args for them\n    learning_rate = 0.0005 # 0.001 -> 0.0005\n    loss_function = 'binary_crossentropy' # Changed from 'categorical_crossentropy' because of 2-class problem\n    optim = tf.keras.optimizers.Adam(learning_rate=learning_rate,\n                                     clipnorm = 1.0) # Changed from SGD to Adam\n\n    # Take embedding dim and size from emb_matrix\n    embedding_dim = len(emb_matrix[0])\n    num_tokens = len(emb_matrix)\n\n    #num_labels = len(set(Y_train)) <- this is removed by claude\n\n    # Now build the model\n    model = Sequential()\n\n    # Embedding layer -- keeping embeddings frozen (trainable=False)\n    model.add(Embedding(num_tokens,\n                        embedding_dim,\n                        embeddings_initializer=Constant(emb_matrix),\n                        trainable=False))\n\n    # Add LSTM layers with dropout\n#     model.add(Bidirectional(LSTM(units = 100, # 100 -> 128 -> 256 -> 100\n#                    return_sequences = True, # Return full sequence for next layer\n#                    dropout = 0.3, # Dropout on inputs 0.2 -> 0.3\n#                    recurrent_dropout = 0.3,))) # Dropout on recurrent connections 0.2 -> 0.3\n\n#     # Add another LSTM layer\n#     model.add(Bidirectional(LSTM(units = 50, # 50 -> 64 -> 128 -> 50\n#                    dropout = 0.2,\n#                    recurrent_dropout = 0.2,)))\n    \n    model.add((LSTM(units = 100, # 100 -> 128 -> 256 -> 100\n                   return_sequences = True, # Return full sequence for next layer\n                   dropout = 0.3, # Dropout on inputs 0.2 -> 0.3\n                   recurrent_dropout = 0.3,))) # Dropout on recurrent connections 0.2 -> 0.3\n\n    # Add another LSTM layer\n    model.add((LSTM(units = 50, # 50 -> 64 -> 128 -> 50\n                   dropout = 0.2,\n                   recurrent_dropout = 0.2,)))\n\n    # Dense layer -- Ultimately, end with dense layer with softmax\n    model.add(Dense(50,\n                   activation='relu',\n                   kernel_regularizer=tf.keras.regularizers.l2(0.01)))  # Fixed syntax\n    model.add(Dropout(0.3))\n\n    model.add(Dense(32,\n                   activation='relu',\n                   kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n    model.add(Dropout(0.4))  # Increased dropout before final layer\n\n    # Output layer\n        model.add(Dense(units=1, activation=\"sigmoid\"))\n\n    # Compile model using our settings, check for accuracy\n    model.compile(loss=loss_function,\n                  optimizer=optim,\n                  metrics=['accuracy'])\n    return model","metadata":{"id":"BiiVcbfkC9dE","execution":{"iopub.status.busy":"2024-11-02T14:01:49.424684Z","iopub.execute_input":"2024-11-02T14:01:49.425003Z","iopub.status.idle":"2024-11-02T14:01:49.435682Z","shell.execute_reply.started":"2024-11-02T14:01:49.424972Z","shell.execute_reply":"2024-11-02T14:01:49.434957Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## 6. Model training","metadata":{"id":"pGn-bQWsDBD-"}},{"cell_type":"code","source":"def train_model(model, X_train, Y_train, X_dev, Y_dev):\n    '''Train the model here. Note the different settings you can experiment with!'''\n    # Potentially change these to cmd line args again\n    # And yes, don't be afraid to experiment!\n    verbose = 1\n    batch_size = 32 # 16 -> 32 -> 64 -> 32\n    epochs = 50 # 50 -> 10 -> 50 -> 30 -> 50\n\n    # Add learning rate scheduling\n    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                                        factor = 0.1, # 0.5 -> 0.2 -> 0.1\n                                                        patience = 2,\n                                                        verbose = 1,\n                                                        min_lr = 1e-6,\n                                                        min_delta = 0.0001,\n                                                        )\n\n    # Create callback to stop training early if no improvement\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"best_model.keras\",\n                                                       save_best_only=True,\n                                                       monitor='val_accuracy',) # Use val_accuracy to monitor\n\n    # Early stopping callback\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=7,\n                                                         monitor='val_accuracy',\n                                                         restore_best_weights = True)\n\n    # Add class weights\n    class_weights = compute_class_weight(class_weight='balanced',\n                                         classes=np.unique(Y_train),\n                                         y=Y_train)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Finally fit the model to our data\n    history = model.fit(\n        X_train, Y_train,\n        class_weight = class_weight_dict,\n        verbose=verbose,\n        epochs=epochs,\n        callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler],\n        batch_size=batch_size,\n        validation_data=(X_dev, Y_dev))\n\n\n\n    return model, history","metadata":{"id":"FvFuAVvwDG12","execution":{"iopub.status.busy":"2024-11-02T14:01:49.437071Z","iopub.execute_input":"2024-11-02T14:01:49.437432Z","iopub.status.idle":"2024-11-02T14:01:49.449361Z","shell.execute_reply.started":"2024-11-02T14:01:49.437391Z","shell.execute_reply":"2024-11-02T14:01:49.448597Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 7. Testing and evaluating predictions","metadata":{"id":"1skvaz4yDIdq"}},{"cell_type":"code","source":"def test_set_predict(model, X_test, Y_test, ident):\n    '''Do predictions and measure accuracy on our own test set (that we split off train)'''\n    # Get predictions using the trained model\n    Y_pred = model.predict(X_test)\n\n    # Finally, convert to numerical labels to get scores with sklearn\n    Y_pred_binary = (Y_pred > 0.5).astype(int)\n\n    # Calculate accuracy\n    accuracy = accuracy_score(Y_test, Y_pred_binary)\n\n    # Calculate F1 score\n    f1 = f1_score(Y_test, Y_pred_binary, average='macro')\n    precision = precision_score(Y_test, Y_pred_binary, average='macro')\n    recall = recall_score(Y_test, Y_pred_binary, average='macro')\n\n    # Print out the results\n    print(f'Results on {ident} set:')\n    print(f\"Accuracy on {ident} set: {accuracy:.4f}\")\n    print(f\"Macro F1 score on {ident} set: {f1:.4f}\")\n    print(f\"Macro Recall score on {ident} set: {recall:.4f}\")\n    print(f\"Macro Precision score on {ident} set: {precision:.4f}\")\n    print('\\nClassification Report:')\n    print(classification_report(Y_test, Y_pred_binary))\n\n    return accuracy, f1","metadata":{"id":"TEPUV_6NDTNP","execution":{"iopub.status.busy":"2024-11-02T14:01:49.451846Z","iopub.execute_input":"2024-11-02T14:01:49.452143Z","iopub.status.idle":"2024-11-02T14:01:49.462154Z","shell.execute_reply.started":"2024-11-02T14:01:49.452113Z","shell.execute_reply":"2024-11-02T14:01:49.461291Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 8. Main execution","metadata":{"id":"lCdAA4umDVys"}},{"cell_type":"code","source":"\ndef main():\n    '''Main function to train and test neural network given cmd line arguments'''\n\n    # Read in the data and embeddings\n    X_train, Y_train = read_corpus(TRAIN_PATH)\n    X_dev, Y_dev = read_corpus(DEV_PATH)\n    X_test, Y_test = read_corpus(DEV_PATH)\n\n    print('Reading embeddings...')\n    embeddings = read_embeddings_from_txt(EMBEDDINGS_PATH)\n\n    # Transform words to indices using a vectorizer\n    print(\"Vectorizing data...\")\n    vectorizer = TextVectorization(standardize='lower_and_strip_punctuation',\n                                   output_sequence_length=75) # Needs to adjust based on test results\n\n    # Use train and dev to create vocab - could also do just train\n    text_ds = tf.data.Dataset.from_tensor_slices(X_train + X_dev + X_test)\n    vectorizer.adapt(text_ds)\n\n    # Dictionary mapping words to idx\n    voc = vectorizer.get_vocabulary()\n\n    # Get embedding matrix\n    emb_matrix = get_emb_matrix(voc, embeddings)\n\n    # Transform string labels to one-hot encodings (binary)\n    label_dict = {'NOT': 0, 'OFF': 1}\n    Y_train =  np.array([label_dict[y] for y in Y_train])  # NOT Use encoder.classes_ to find mapping back\n    Y_dev = np.array([label_dict[y] for y in Y_dev])\n    Y_test = np.array([label_dict[y] for y in Y_test])\n\n    # Vectorize text data\n    X_train_vect = vectorizer(np.array([[s] for s in X_train])).numpy()\n    X_dev_vect = vectorizer(np.array([[s] for s in X_dev])).numpy()\n    X_test_vect = vectorizer(np.array([[s] for s in X_test])).numpy()\n\n    # Create model\n    #model = create_model(Y_train, emb_matrix) <- Claude removed it\n\n    # Transform input to vectorized input\n    # Create and train model\n    print('Creating model...')\n    model = create_model(Y_train, emb_matrix)\n    print('Training model...')\n    model, history = train_model(model, X_train_vect, Y_train, X_dev_vect, Y_dev)\n\n    # Train the model\n    #model = train_model(model, X_train_vect, Y_train_bin, X_dev_vect, Y_dev_bin) <- clause removed it\n\n    return model, history, X_dev_vect, Y_dev, X_test_vect, Y_test\n\n\n\n","metadata":{"id":"4dzA0ek-_8qA","execution":{"iopub.status.busy":"2024-11-02T14:01:49.463402Z","iopub.execute_input":"2024-11-02T14:01:49.463762Z","iopub.status.idle":"2024-11-02T14:01:49.475504Z","shell.execute_reply.started":"2024-11-02T14:01:49.463722Z","shell.execute_reply":"2024-11-02T14:01:49.474732Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"\n # Do predictions on specified test set -- test if specified","metadata":{"id":"YIaZlh3KA9Ob"}},{"cell_type":"code","source":"# # Do predictions on specified test set -- test if specified\n# if TEST_PATH:\n#     # Read in test set and vectorize\n#     print('Predicting on test set...')\n#     X_test, Y_test = read_corpus(TEST_PATH)\n#     Y_test = np.array([label_dict[y] for y in Y_test])\n#     X_test_vect = vectorizer(np.array([[s] for s in X_test])).numpy()\n\n#     # Finally do the predictions\n#     test_set_predict(model, X_test_vect, Y_test, \"test\")","metadata":{"id":"qyho6JS8B3Zh","execution":{"iopub.status.busy":"2024-11-02T14:01:49.476469Z","iopub.execute_input":"2024-11-02T14:01:49.476748Z","iopub.status.idle":"2024-11-02T14:01:49.487495Z","shell.execute_reply.started":"2024-11-02T14:01:49.476719Z","shell.execute_reply":"2024-11-02T14:01:49.486673Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 9. Add visualisation to training history","metadata":{"id":"YCSAoXLKM1vh"}},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"Plot training history\"\"\"\n    import matplotlib.pyplot as plt\n\n    # Plot training & validation accuracy values\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n\n    # Plot training & validation loss values\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"56-CGewZAMua","execution":{"iopub.status.busy":"2024-11-02T14:01:49.488379Z","iopub.execute_input":"2024-11-02T14:01:49.488627Z","iopub.status.idle":"2024-11-02T14:01:49.496927Z","shell.execute_reply.started":"2024-11-02T14:01:49.488599Z","shell.execute_reply":"2024-11-02T14:01:49.496151Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Results & Evaluation\n","metadata":{"id":"3Y6eFavaW1jm"}},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Train model and get history\n    model, history, X_dev_vect, Y_dev, X_test_vect, Y_test = main()\n\n    # Get final evaluation results\n    print(\"\\nFinal Evaluation Results\")\n    test_set_predict(model, X_test_vect, Y_test, 'test')\n\n    # Plot training history\n    plot_training_history(history)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mi5uyuVNM7PF","outputId":"4c11e2be-70cd-4b15-dd2b-13f07c3921d2","execution":{"iopub.status.busy":"2024-11-02T14:01:49.497878Z","iopub.execute_input":"2024-11-02T14:01:49.498136Z","iopub.status.idle":"2024-11-02T14:02:24.299917Z","shell.execute_reply.started":"2024-11-02T14:01:49.498107Z","shell.execute_reply":"2024-11-02T14:02:24.297666Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Reading corpus from /kaggle/input/off-set-rnn/train.tsv\nRead 12240 documents\nLabel distribution: NOT    8192\nOFF    4048\nName: count, dtype: int64\nReading corpus from /kaggle/input/off-set-rnn/dev.tsv\nRead 1000 documents\nLabel distribution: NOT    648\nOFF    352\nName: count, dtype: int64\nReading corpus from /kaggle/input/off-set-rnn/dev.tsv\nRead 1000 documents\nLabel distribution: NOT    648\nOFF    352\nName: count, dtype: int64\nReading embeddings...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Train model and get history\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model, history, X_dev_vect, Y_dev, X_test_vect, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Get final evaluation results\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Evaluation Results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m X_test, Y_test \u001b[38;5;241m=\u001b[39m read_corpus(DEV_PATH)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading embeddings...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mread_embeddings_from_txt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDINGS_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Transform words to indices using a vectorizer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorizing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mread_embeddings_from_txt\u001b[0;34m(embeddings_file)\u001b[0m\n\u001b[1;32m     11\u001b[0m         values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     12\u001b[0m         word \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m         vector \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         embeddings[word] \u001b[38;5;241m=\u001b[39m vector\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}