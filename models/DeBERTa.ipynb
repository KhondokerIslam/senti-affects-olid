{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":9765865,"sourceType":"datasetVersion","datasetId":5980971},{"sourceId":9786962,"sourceType":"datasetVersion","datasetId":5996551}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom collections import Counter\nimport pandas as pd\nimport re\nimport string\nimport emoji\n# import contractions\nfrom nltk.corpus import stopwords\nimport nltk\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Clean text function\n# def clean_text(text):\n#     text = text.lower()\n#     text = contractions.fix(text)\n#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n#     text = re.sub(r'@\\w+', '<USER>', text)\n#     text = re.sub(r'#(\\w+)', r'\\1', text)\n#     text = emoji.demojize(text)\n#     text = re.sub(r\"([a-z])\\1{2,}\", r\"\\1\\1\", text)\n#     text = text.translate(str.maketrans('', '', string.punctuation))\n#     text = re.sub(r'\\s+', ' ', text).strip()\n#     text = \" \".join([word for word in text.split() if word not in stop_words])\n#     return text\n\n# Load and preprocess data\n# train_data = pd.read_csv('/kaggle/input/offensive-semeval-task-1/train.tsv', sep='\\t' )\n# dev_data = pd.read_csv('/kaggle/input/offensive-semeval-task-1/dev.tsv', sep='\\t')\n# test_data = pd.read_csv('/kaggle/input/offensive-semeval-task-1/test.tsv', sep='\\t', names = ['Data', 'Label'])\n\n# Load senti append data\ntrain_data = pd.read_csv('/kaggle/input/sentiment-append-off-set/mrm_senti_append_train.tsv', sep='\\t')\ndev_data = pd.read_csv('/kaggle/input/sentiment-append-off-set/mrm_senti_append_dev.tsv', sep='\\t')\ntest_data = pd.read_csv('/kaggle/input/sentiment-append-off-set/mrm_senti_append_test.tsv', sep='\\t')\n\nX_train, y_train = train_data.iloc[:, 0], train_data.iloc[:, 1]\nX_dev, y_dev = dev_data.iloc[:, 0], dev_data.iloc[:, 1]\nX_test, y_test = test_data.iloc[:, 0], test_data.iloc[:, 1]\n\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(y_train)\ny_dev = label_encoder.transform(y_dev)\ny_test = label_encoder.transform(y_test)\n\n# Load DeBERTa model and tokenizer\n# lm = \"microsoft/deberta-v3-large\"\nlm = \"microsoft/deberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(lm)\n\ndef tokenize_inputs(texts):\n    return tokenizer(texts, padding=True, truncation=True, max_length=77, return_tensors=\"pt\")\n\ntrain_encodings = tokenize_inputs(X_train.tolist())\ndev_encodings = tokenize_inputs(X_dev.tolist())\ntest_encodings = tokenize_inputs(X_test.tolist())\n\ny_train_tensor = torch.tensor(y_train)\ny_dev_tensor = torch.tensor(y_dev)\ny_test_tensor = torch.tensor(y_test)\n\nbatch_size = 16\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], y_train_tensor)\ndev_dataset = TensorDataset(dev_encodings['input_ids'], dev_encodings['attention_mask'], y_dev_tensor)\ntest_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Focal loss function for handling class imbalance\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        BCE_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n        return F_loss.mean() if self.reduction == 'mean' else F_loss.sum()\n\n    \n# Multi-task learning model setup\n# Multi-task learning model setup\nclass DeBERTaModel(torch.nn.Module):\n    def __init__(self, model_name, num_main_labels):\n        super(DeBERTaModel, self).__init__()\n        # Load the DeBERTa model with hidden states enabled\n        self.bert = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=num_main_labels,\n            output_hidden_states=False\n#             output_hidden_states=True  # Enable hidden states\n        )\n        \n    def forward(self, input_ids, attention_mask, labels=None, aux_labels=None):\n        # Pass inputs through the DeBERTa model\n        outputs = self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n        logits = outputs.logits\n        #hidden_states = outputs.hidden_states  # Retrieve the hidden states\n\n        ## Use the last layer's hidden states and average across tokens\n        #aux_logits = self.aux_classifier(hidden_states[-1].mean(dim=1))\n\n        #return logits, aux_logits\n        return logits\n\n\nnum_labels = len(label_encoder.classes_)\nmodel = DeBERTaModel(model_name=lm, num_main_labels=num_labels)\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.3)\nnum_epochs = 5\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# Training with Focal Loss and Early Stopping\nloss_fn = FocalLoss(alpha=1.0, gamma=2.0).to(device)\nearly_stop_tolerance = 2\nno_improvement_epochs = 0\nbest_f1 = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n        #logits, aux_logits = model(input_ids, attention_mask=attention_mask, labels=labels)\n        logits = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_loader)}\")\n\n    # Validation phase\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in dev_loader:\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            #logits, aux_logits = model(input_ids, attention_mask=attention_mask)\n            logits = model(input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    recall = recall_score(all_labels, all_preds, average='macro')\n    precision = precision_score(all_labels, all_preds, average='macro')\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    print(f'Validation - Accuracy: {accuracy:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f} | F1 Score: {f1:.4f}')\n\n    if f1 > best_f1:\n        best_f1 = f1\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n        if no_improvement_epochs >= early_stop_tolerance:\n            print(\"Early stopping triggered\")\n            break\n\n# Test Evaluation\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n        #logits, aux_logits = model(input_ids, attention_mask=attention_mask)\n        logits = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\naccuracy = accuracy_score(all_labels, all_preds)\nrecall = recall_score(all_labels, all_preds, average='macro')\nprecision = precision_score(all_labels, all_preds, average='macro')\nf1 = f1_score(all_labels, all_preds, average='macro')\n\ntest_data['actual_label'] = all_labels\ntest_data['predicted_label'] = all_preds\n\ntest_data.to_csv( 'pps_olid_test_outputs.tsv', sep='\\t', index=False)\n\nprint(f'Test - Accuracy: {accuracy:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f} | F1 Score: {f1:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:39:08.775650Z","iopub.execute_input":"2024-11-03T09:39:08.776064Z","iopub.status.idle":"2024-11-03T09:54:26.920774Z","shell.execute_reply.started":"2024-11-03T09:39:08.776024Z","shell.execute_reply":"2024-11-03T09:54:26.919592Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eccb429e4e0e40a3bb92db4b91dcc918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be9ef528b726408f87be7aae449a0fdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b602842c881472ca2afa353ada93f56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389d15588f254f07a18b3ac950085f13"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6144ea2154e84853bca092c20db8796d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training Loss: 0.1507037171010488\nValidation - Accuracy: 0.7470 | Recall: 0.7710 | Precision: 0.7473 | F1 Score: 0.7414\nEpoch 2 - Training Loss: 0.11539890218550473\nValidation - Accuracy: 0.8180 | Recall: 0.7953 | Precision: 0.8019 | F1 Score: 0.7984\nEpoch 3 - Training Loss: 0.0967629640539682\nValidation - Accuracy: 0.7970 | Recall: 0.7921 | Precision: 0.7783 | F1 Score: 0.7834\nEpoch 4 - Training Loss: 0.07914789376256902\nValidation - Accuracy: 0.7960 | Recall: 0.7784 | Precision: 0.7762 | F1 Score: 0.7773\nEarly stopping triggered\nTest - Accuracy: 0.8442 | Recall: 0.8013 | Precision: 0.8077 | F1 Score: 0.8044\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}